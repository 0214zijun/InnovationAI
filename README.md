# Innovation AI – 12-Week Quantitative Finance & Machine Learning Internship

**Author:** Zijun Qiu  
**Internship Duration:** July 8 – September 27, 2025  
**Organization:** Innovation AI  
**Role:** Quantitative Finance & Machine Learning Intern  



```text
print("""
            _ooOoo_
           o888888o
           88" . "88
           (| -_- |)
            O\ = /O
        ____/`---'\____
      .'  \\|     |//  `.
     /  \\|||  :  |||//  \
    |  _||||| -:- |||||_  |
    |   | \\\  -  /// |   |
    | \_|  ''\---/''  |_/ |
     \  .-\__  `-`  ___/-. /
   ___`. .'  /--.--\  `. . __
."" '<  `.___\_<|>_/___.' >'" ".
| | :  `- \`.;`\ _ /`;.`/ - ` : | |
\  \ `-.   \_ __\ /__ _/   .-` /  /
=====`-.____`-.___\_____/___.-`____.-'=====
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ALL BLESSINGS      NEVER BUGGING
""")


# Innovation AI — Quantitative Finance & Machine Learning Internship

**Author:** Zijun Qiu  
**Duration:** July – September 2025  
**Organization:** Innovation AI  
**Role:** Quantitative Finance & Machine Learning Intern  

---

## Project Overview

This repository documents a 12-week quantitative research internship focused on applying **data science, machine learning, and statistical analysis** to U.S. equity markets.

The project centers on building a **data-driven investment analysis pipeline**, integrating market price data, technical indicators, and financial metrics to support research, modeling, and backtesting workflows.

---

## Objectives

- Design a reproducible data pipeline for U.S. equity analysis  
- Engineer technical and statistical features from time-series data  
- Evaluate feature predictiveness using correlation and volatility analysis  
- Support downstream modeling, experimentation, and backtesting  

---

## Data & Feature Engineering

- Collected and processed **100,000+ rows of OHLCV time-series data** using `yfinance`
- Engineered **15+ technical indicators**, including:
  - SMA / EMA
  - RSI
  - MACD
  - Bollinger Bands
- Implemented:
  - Missing value imputation
  - Z-score and Min-Max normalization
  - Multi-source data alignment on a daily frequency

---

## Methodology

- Built preprocessing pipelines using **Python (Pandas, NumPy)**
- Applied correlation and volatility-based screening to:
  - Reduce noise
  - Assess feature relevance
- Structured outputs into **model-ready datasets** for downstream ML workflows
- Ensured reproducibility through versioned scripts and standardized data formats

---

## Repository Structure

